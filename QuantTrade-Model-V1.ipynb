{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c320316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime, timedelta\n",
    "# import pandas as pd\n",
    "# from binance.client import Client\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"API_KEY\")\n",
    "# api_secret = os.getenv(\"API_SECRET\")\n",
    "\n",
    "# client = Client(api_key, api_secret)\n",
    "\n",
    "# three_months_ago = datetime.now() - timedelta(days=90)\n",
    "\n",
    "# klines = client.get_historical_klines(\n",
    "#     symbol='BTCUSDT',\n",
    "#     interval=Client.KLINE_INTERVAL_5MINUTE,\n",
    "#     start_str=three_months_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# )\n",
    "\n",
    "# df = pd.DataFrame(klines, columns=[\n",
    "#     'Open time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close time',\n",
    "#     'Quote asset volume', 'Number of trades', 'Taker buy base asset volume',\n",
    "#     'Taker buy quote asset volume', 'Ignore'\n",
    "# ])\n",
    "\n",
    "# df['Open time'] = pd.to_datetime(df['Open time'], unit='ms')\n",
    "# price_df = df[['Open time', 'Close']].copy()\n",
    "# price_df['Close'] = pd.to_numeric(price_df['Close'])\n",
    "\n",
    "# print(price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cf7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas_ta\\__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from binance.client import Client\n",
    "from dotenv import load_dotenv\n",
    "from warnings import simplefilter\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from xgboost import plot_importance\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21cd9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525230, 6)\n"
     ]
    }
   ],
   "source": [
    "client = Client(api_key, api_secret)\n",
    "one_year_ago = datetime.now() - timedelta(days=365*5)\n",
    "klines = client.get_historical_klines(\n",
    "    symbol='BTCUSDT',\n",
    "    interval=Client.KLINE_INTERVAL_5MINUTE,\n",
    "    start_str=one_year_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(klines, columns=[\n",
    "    'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close time',\n",
    "    'Quote asset volume', 'Number of trades', 'Taker buy base asset volume',\n",
    "    'Taker buy quote asset volume', 'Ignore'\n",
    "])\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Number of trades']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "print(f\"{df.shape}\")\n",
    "df.to_parquet(\"btc_5m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Fractional Differentiation...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"btc_5m_5years.parquet\")\n",
    "\n",
    "def create_volume_bars(df, volume_threshold=1000):\n",
    "    bars = []\n",
    "    current_bar = None\n",
    "    cumulative_volume = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if current_bar is None:\n",
    "            current_bar = {'Date': row['Date'], 'Open': row['Open'], 'High': row['High'], 'Low': row['Low']}\n",
    "        cumulative_volume += row['Volume']\n",
    "        current_bar['High'] = max(current_bar['High'], row['High'])\n",
    "        current_bar['Low'] = min(current_bar['Low'], row['Low'])\n",
    "        if cumulative_volume >= volume_threshold:\n",
    "            current_bar['Close'] = row['Close']\n",
    "            current_bar['Volume'] = cumulative_volume\n",
    "            bars.append(current_bar)\n",
    "            current_bar = None\n",
    "            cumulative_volume = 0\n",
    "    return pd.DataFrame(bars)\n",
    "\n",
    "df_vb = create_volume_bars(df, volume_threshold=1000)\n",
    "\n",
    "def get_weights_ffd(d, size):\n",
    "    w = [1.]\n",
    "    for k in range(1, size):\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        w.append(w_)\n",
    "    return np.array(w[::-1]).reshape(-1, 1)\n",
    "\n",
    "def frac_diff_ffd(series, d, thres=1e-5):\n",
    "    w = get_weights_ffd(d, len(series))\n",
    "    w_ = np.cumsum(np.abs(w))\n",
    "    w_ /= w_[-1]\n",
    "    skip = np.searchsorted(w_, thres)\n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        series_f = series[[name]].ffill().dropna()\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        for iloc in range(skip, series_f.shape[0]):\n",
    "            loc = series_f.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc, name]): continue\n",
    "            df_[loc] = np.dot(w[-(iloc + 1):, :].T, series_f.iloc[:iloc + 1])[0, 0]\n",
    "        df[name] = df_\n",
    "    return pd.concat(df, axis=1)\n",
    "\n",
    "print(\"Applying Fractional Differentiation...\")\n",
    "close_prices_vb = df_vb[['Close']].set_index(df_vb['Date'])\n",
    "frac_close = frac_diff_ffd(close_prices_vb, d=0.4).dropna()\n",
    "df_vb = df_vb.set_index('Date').loc[frac_close.index].reset_index()\n",
    "df_vb['frac_close'] = frac_close['Close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec10d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features...\n"
     ]
    }
   ],
   "source": [
    "def calculate_features(df):\n",
    "    print(\"Calculating features...\")\n",
    "    df.ta.adx(length=14, append=True)\n",
    "    df.ta.aroon(length=25, append=True)\n",
    "    df.ta.bbands(length=20, append=True)\n",
    "    df.ta.atr(length=14, append=True)\n",
    "    df.ta.obv(append=True)\n",
    "    df['slope_24'] = df['Close'].rolling(window=24).apply(lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if x.notna().all() else np.nan, raw=False)\n",
    "    df['bbw_pct'] = (df['BBU_20_2.0'] - df['BBL_20_2.0']) / df['BBM_20_2.0']\n",
    "    df.ta.chop(length=14, append=True)\n",
    "    df['clv'] = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'] + 1e-12)\n",
    "    df['clv_mean_12'] = df['clv'].rolling(window=12).mean()\n",
    "    return df\n",
    "\n",
    "df_features = calculate_features(df_vb.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d193cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating regime labels...\n"
     ]
    }
   ],
   "source": [
    "def get_regime_labels(close_series, look_forward_period=20):\n",
    "    print(\"Creating regime labels...\")\n",
    "    future_vol = close_series.shift(-look_forward_period).rolling(look_forward_period).std()\n",
    "    trend_cutoff = future_vol.quantile(0.66)\n",
    "    sideway_cutoff = future_vol.quantile(0.33)\n",
    "    labels = pd.Series(index=close_series.index, data=-1, dtype=int)\n",
    "    labels[future_vol >= trend_cutoff] = 1\n",
    "    labels[future_vol <= sideway_cutoff] = 0\n",
    "    return labels\n",
    "\n",
    "df_features['regime'] = get_regime_labels(df_features['Close'])\n",
    "df_model_data = df_features[df_features['regime'] != -1].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f50b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for the model...\n",
      "Training data shape: (55172, 17)\n",
      "Test data shape: (13794, 17)\n",
      "Shape before dropping NaNs from features: (102958, 26)\n",
      "Shape after dropping NaNs: (102958, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data for the model...\")\n",
    "X = df_model_data.drop(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'clv', 'regime', 'frac_close'])\n",
    "y = df_model_data['regime']\n",
    "\n",
    "test_size = 0.2\n",
    "split_index = int(len(X) * (1 - test_size))\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Shape before dropping NaNs from features: {df_features.shape}\")\n",
    "df_features.dropna(inplace=True)\n",
    "print(f\"Shape after dropping NaNs: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cached file: btc_5m_5years.parquet\n",
      "Creating Volume Bars with threshold: 2000 BTC...\n",
      "Applying Fractional Differentiation to the 'Close' price...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from binance.client import Client\n",
    "from dotenv import load_dotenv\n",
    "from warnings import simplefilter\n",
    "from xgboost import plot_importance\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "\n",
    "def download_and_cache_data(file_path=\"btc_5m_5years.parquet\", years=5):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data from cached file: {file_path}\")\n",
    "        return pd.read_parquet(file_path)\n",
    "    print(\"Downloading historical data (this may take a while)...\")\n",
    "    client = Client(api_key, api_secret)\n",
    "    start_date = datetime.now() - timedelta(days=365 * years)\n",
    "    klines = client.get_historical_klines(\n",
    "        symbol='BTCUSDT',\n",
    "        interval=Client.KLINE_INTERVAL_5MINUTE,\n",
    "        start_str=start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close time',\n",
    "        'Quote asset volume', 'Number of trades', 'Taker buy base asset volume',\n",
    "        'Taker buy quote asset volume', 'Ignore'\n",
    "    ])\n",
    "    df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "    df.to_parquet(file_path)\n",
    "    return df\n",
    "\n",
    "df_time_bars = download_and_cache_data()\n",
    "\n",
    "def create_volume_bars(df, volume_threshold=1000):\n",
    "    print(f\"Creating Volume Bars with threshold: {volume_threshold} BTC...\")\n",
    "    bars = []\n",
    "    current_bar = None\n",
    "    cumulative_volume = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if current_bar is None:\n",
    "            current_bar = {'Date': row['Date'], 'Open': row['Open'], 'High': row['High'], 'Low': row['Low']}\n",
    "        cumulative_volume += row['Volume']\n",
    "        current_bar['High'] = max(current_bar['High'], row['High'])\n",
    "        current_bar['Low'] = min(current_bar['Low'], row['Low'])\n",
    "        if cumulative_volume >= volume_threshold:\n",
    "            current_bar['Close'] = row['Close']\n",
    "            current_bar['Volume'] = cumulative_volume\n",
    "            bars.append(current_bar)\n",
    "            current_bar = None\n",
    "            cumulative_volume = 0\n",
    "    return pd.DataFrame(bars).set_index('Date')\n",
    "\n",
    "df_vb = create_volume_bars(df_time_bars, volume_threshold=2000)\n",
    "\n",
    "def get_weights_ffd(d, size):\n",
    "    w = [1.]\n",
    "    for k in range(1, size):\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        w.append(w_)\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w\n",
    "\n",
    "def frac_diff_ffd(series, d, thres=1e-5):\n",
    "    w = get_weights_ffd(d, len(series))\n",
    "    w_ = np.cumsum(np.abs(w))\n",
    "    w_ /= w_[-1]\n",
    "    skip = np.searchsorted(w_, thres)\n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        series_f = series[[name]].ffill().dropna()\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        for iloc in range(skip, series_f.shape[0]):\n",
    "            loc = series_f.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc, name]):\n",
    "                continue\n",
    "            df_[loc] = np.dot(w[-(iloc + 1):, :].T, series_f.iloc[:iloc + 1])[0, 0]\n",
    "        df[name] = df_\n",
    "    return pd.concat(df, axis=1)\n",
    "\n",
    "print(\"Applying Fractional Differentiation to the 'Close' price...\")\n",
    "frac_close = frac_diff_ffd(df_vb[['Close']], d=0.4)\n",
    "frac_close.rename(columns={'Close': 'frac_close'}, inplace=True)\n",
    "df_vb = df_vb.join(frac_close)\n",
    "print(\"Fractional Differentiation complete.\")\n",
    "\n",
    "def get_daily_vol(close, lookback=100):\n",
    "    ret = close.pct_change()\n",
    "    vol = ret.ewm(span=lookback).std()\n",
    "    return vol\n",
    "\n",
    "def apply_pt_sl_on_t1(close, events, pt_sl, molecule):\n",
    "    events_ = events.loc[molecule]\n",
    "    out = events_[['t1']].copy(deep=True)\n",
    "    pt_mult, sl_mult = pt_sl\n",
    "    pt = events_['trgt'] * pt_mult\n",
    "    sl = events_['trgt'] * sl_mult\n",
    "    for loc, t1 in events_['t1'].items():\n",
    "        df0 = close[loc:t1]\n",
    "        df0 = (df0 / close[loc] - 1)\n",
    "        out.loc[loc, 'sl'] = df0[df0 < -sl[loc]].index.min()\n",
    "        out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()\n",
    "    return out\n",
    "\n",
    "def get_events(close, t_events, pt_sl, trgt, min_ret, t1=None):\n",
    "    trgt = trgt.loc[t_events]\n",
    "    trgt = trgt[trgt > min_ret]\n",
    "    if t1 is None:\n",
    "        t1 = pd.Series(pd.NaT, index=t_events)\n",
    "    events = pd.concat({'t1': t1, 'trgt': trgt}, axis=1).dropna(subset=['trgt'])\n",
    "    df0 = apply_pt_sl_on_t1(close, events, pt_sl, events.index)\n",
    "    events['t1'] = df0.min(axis=1)\n",
    "    return events\n",
    "\n",
    "def get_bins(events, close):\n",
    "    events_ = events.dropna(subset=['t1'])\n",
    "    px = close.reindex(events_.index.union(events_['t1'].values)).ffill()\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "    out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "    out['bin'] = np.sign(out['ret'])\n",
    "    out.loc[out['bin'] == 0, 'bin'] = -1\n",
    "    return out\n",
    "\n",
    "print(\"\\nStarting Triple-Barrier Labeling...\")\n",
    "volatility = get_daily_vol(df_vb['Close'], lookback=50)\n",
    "t_events = df_vb.index[50:]\n",
    "vertical_barrier = t_events + pd.Timedelta(days=5)\n",
    "vertical_barrier = pd.Series(vertical_barrier[vertical_barrier < df_vb.index[-1]], index=t_events[:len(vertical_barrier)])\n",
    "events = get_events(df_vb['Close'], t_events, [2.0, 1.0], volatility, 0.001, vertical_barrier)\n",
    "labels = get_bins(events, df_vb['Close'])\n",
    "print(\"Labeling complete.\")\n",
    "\n",
    "def calculate_all_features(df):\n",
    "    print(\"Calculating features...\")\n",
    "    df.ta.adx(length=14, append=True)\n",
    "    df.ta.aroon(length=25, append=True)\n",
    "    df.ta.bbands(length=20, append=True)\n",
    "    df['bbw_pct'] = (df['BBU_20_2.0'] - df['BBL_20_2.0']) / df['BBM_20_2.0']\n",
    "    df.ta.atr(length=14, append=True)\n",
    "    df.ta.chop(length=14, append=True)\n",
    "    df.ta.obv(append=True)\n",
    "    return df\n",
    "\n",
    "df_features = calculate_all_features(df_vb.copy())\n",
    "\n",
    "data = df_features.join(labels['bin']).dropna()\n",
    "X = data.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'bin'])\n",
    "y = data['bin']\n",
    "y[y <= 0] = 0\n",
    "print(f\"Final dataset shape for modeling: {X.shape}\")\n",
    "\n",
    "class PurgedKFold(KFold):\n",
    "    def __init__(self, n_splits=10, t1=None, pct_embargo=0.):\n",
    "        super().__init__(n_splits, shuffle=False)\n",
    "        self.t1 = t1\n",
    "        self.pct_embargo = pct_embargo\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        embargo = int(X.shape[0] * self.pct_embargo)\n",
    "        test_splits = [(i[0], i[-1] + 1) for i in np.array_split(indices, self.n_splits)]\n",
    "        for i, j in test_splits:\n",
    "            test_indices = indices[i:j]\n",
    "            t0 = X.index[i]\n",
    "            train_indices = []\n",
    "            for ix in X.index:\n",
    "                if ix < t0 and self.t1.loc[ix] < t0:\n",
    "                    train_indices.append(X.index.get_loc(ix))\n",
    "            if len(test_indices) > 0:\n",
    "                t1_test_max = self.t1.iloc[test_indices].max()\n",
    "                embargo_start_time = t1_test_max + pd.Timedelta(minutes=embargo * 5)\n",
    "                train_indices = [idx for idx in train_indices if X.index[idx] > embargo_start_time or X.index[idx] < t0]\n",
    "            yield np.array(train_indices), test_indices\n",
    "\n",
    "print(\"\\nStarting Purged K-Fold Cross-Validation...\")\n",
    "cv_splitter = PurgedKFold(n_splits=5, t1=events['t1'], pct_embargo=0.01)\n",
    "oos_scores = []\n",
    "for train_idx, test_idx in cv_splitter.split(X):\n",
    "    if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "        continue\n",
    "    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    scale_pos_weight = y_train.value_counts().get(0, 1) / y_train.value_counts().get(1, 1)\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "        n_estimators=500, learning_rate=0.01, max_depth=4,\n",
    "        random_state=42, scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)\n",
    "    y_pred = model.predict(X_test)\n",
    "    oos_scores.append(f1_score(y_test, y_pred))\n",
    "    print(f\"Fold F1-Score: {oos_scores[-1]:.4f}\")\n",
    "print(f\"\\nAverage F1-Score from Purged K-Fold CV: {np.mean(oos_scores):.4f}\")\n",
    "\n",
    "print(\"\\nTraining final model...\")\n",
    "final_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "    n_estimators=500, learning_rate=0.01, max_depth=4, random_state=42\n",
    ")\n",
    "final_model.fit(X, y, verbose=False)\n",
    "print(\"Plotting Feature Importance...\")\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_importance(final_model, ax=ax, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
